# python scripts
 * preprocessing/:
 * wiki-crawler/: The wiki-crawler was an attempt to crawl all medicine related Wikipedia pages for building a corpus which then could have been used to build domain specific word embeddings. It should have been starting at the portal page for medicine and then following each link it finds. But this procedure was never implemented nor fully used. Due to time constraints we decided to use a pre-computed word embeddings model from the [GloVe project](https://nlp.stanford.edu/projects/glove/).
